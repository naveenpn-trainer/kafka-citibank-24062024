# Getting Started with Big Data and Framework

> Big data is a term which is used to describe a massive volume of data which is comes with high velocity, complex type of structured , semi-structured and unstructured, difficult to store and process using a traditional processing system (RDBMS)

**Two main challenges of Big Data**

1. Storage - Distributed Storage System
2. Processing - Massive Parallel Processing (MPP)

## Hadoop (2.x)

To address the challenges of Big Data

> **Hadoop** is a framework which allows to s**tore and process massive volume** of data in parallel and distributed fashion.

Hadoop consists of 3 main components

1. Storage Layer - Hadoop Distributed File System (HDFS)
2. Resource Management Layer - YARN
3. Data Processing Layer - MapReduce V2

![img](https://lh7-us.googleusercontent.com/docsz/AD_4nXc82FNPrmqYNSzJeJc09AioaW7-0nMrH71yJcCG3bwv3AFH3xFWf1FrGjnbetTEJzPpdvevKsEdPr8Saqh3JlOdJaOziNJmpP-3wwx3bt5xl3EGItM4M_3h7DkZLKDYjl_RZZui2O-NjQ-7WsbG1PhnaqSK?key=KG4XycolQz2vWFq2bNIfEQ)

![image-20240624105934231](I:\My Drive\Training Providers\Corporate Trainings\Imarticus\CitiBank\Batch 04 - Kafka\kafka-citibank-24062024\imgs\01. Getting Started with Big Data and Frameworks\image-20240624105934231.png)

## Apache Spark

> Apache Spark is an **in-memory cluster computing framework** designed to handle <u>wide range of big data workload</u>s

1. Data Integration and ETL
2. High Performance Batch Computation
3. Machine Learning Analytics
4. GraphX Computation
5. Real-time 